# MCP Server connection
quarkus.langchain4j.mcp.weather.transport-type=streamable-http
quarkus.langchain4j.mcp.weather.url=http://localhost:9080/mcp-liberty-server/mcp

quarkus.http.port=8080
%dev.quarkus.http.port=8080

# Debug
quarkus.langchain4j.log-requests=true
quarkus.langchain4j.log-responses=true

# Ollama settings
quarkus.langchain4j.chat-model.provider=ollama
langchain4j-ollama-dev-service.ollama.host=localhost
langchain4j-ollama-dev-service.ollama.port=11434
langchain4j-ollama-dev-service.ollama.endpoint=http://${langchain4j-ollama-dev-service.ollama.host}:${langchain4j-ollama-dev-service.ollama.port}
quarkus.langchain4j.ollama.chat-model.model-id=gpt-oss:20b
quarkus.langchain4j.ollama.chat-model.temperature=.6
quarkus.langchain4j.ollama.chat-model.top-p=.6
quarkus.langchain4j.ollama.chat-model.top-k=30
quarkus.langchain4j.ollama.timeout=120s

# OpenAI settings
# quarkus.langchain4j.chat-model.provider=openai
# quarkus.langchain4j.openai.api-key=${OPENAI_API_KEY}
# quarkus.langchain4j.openai.chat-model.model-name=gpt-5-mini

# Anthropic settings
# quarkus.langchain4j.chat-model.provider=anthropic
# quarkus.langchain4j.anthropic.api-key=${ANTHROPIC_API_KEY}
# quarkus.langchain4j.anthropic.chat-model.model-name=claude-sonnet-4-5

# Timeouts

quarkus.langchain4j.timeout=120s
quarkus.vertx.max-worker-execute-time=120s